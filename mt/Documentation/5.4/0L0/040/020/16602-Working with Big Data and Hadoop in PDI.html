<content type="text/html" title="Working with Big Data and Hadoop in PDI">
  <body>
    <div>
      <div class="pentaho-overview-hidden">
        <div id="section_1" class="mt-page-section"><span id="Overview"></span> 
          <h2 class="editable">Overview</h2>
          <p>Explains how to work with Big Data and Hadoop in PDI.</p>
        </div>
      </div>
      <div class="body">
        <p class="p">Pentaho Data Integration (PDI) can operate in two distinct modes, job orchestration and data transformation. Within PDI they are referred to as jobs and transformations.</p>
        <p class="p">PDI jobs sequence a set of entries that encapsulate actions. An example of a PDI big data job would be to check for existence of new log files, copy the new files to HDFS, execute a MapReduce task to aggregate the weblog into a click stream and
          stage that clickstream data in an analytic database.</p>
        <p class="p">PDI transformations consist of a set of steps that execute in parallel and operate on a stream of data columns. The columns usually flow from one system, through the PDI engine, where new columns can be calculated or values can be looked up and
          added to the stream. The data stream is then sent to a receiving system like a Hadoop cluster, a database, or even the Pentaho Reporting Engine.&nbsp;</p>
        <p class="p">The tutorials within this section illustrate how to use PDI jobs and transforms in typical big data scenarios. PDI job entries and transformation steps are described in the
          <a class="xref" title="Documentation/5.1/0Y0/0K0" rel="internal" href="http://help.pentaho.com/Documentation/5.4/0L0/0Y0/0K0">Transformation Step Reference</a>  and
          <a class="xref" title="Documentation/5.1/0Y0/0L0" rel="internal" href="http://help.pentaho.com/Documentation/5.4/0L0/0Y0/0L0">Job Entry Reference</a>  sections of Administer the DI Server.</p>
        <div class="section">
          <div id="section_2" class="mt-page-section"><span id="PDI's_Big_Data_Plugin"></span> 
            <h2 class="editable">PDI's Big Data Plugin</h2>
            <p class="p">The Pentaho Big Data plugin contains all of the job entries and transformation steps required for working with Hadoop, Cassandra, and MongoDB.</p>
            <p class="p">By default, PDI is pre-configured to work with Apache Hadoop 0.20.X. But PDI can be configured to communicate with most popular Hadoop distributions. Instructions for changing Hadoop configurations are covered in the
              <a class="xref" title="Documentation/5.1/040/010"
              rel="internal" href="http://help.pentaho.com/Documentation/5.4/0L0/040/010">Configure Your Big Data Environment</a>  section.</p>
            <p class="p">For a list of supported big data technology, including which configurations of Hadoop are currently supported, see the section on
              <a class="xref" title="Components Reference" rel="internal" href="http://help.pentaho.com/Documentation/5.4/0D0/160/000">Supported Components</a> .</p>
          </div>
        </div>
        <div class="section">
          <div id="section_3" class="mt-page-section"><span id="Using_PDI_Outside_and_Inside_the_Hadoop_Cluster"></span> 
            <h2 class="editable">Using PDI Outside and Inside the Hadoop Cluster</h2>
            <p class="p">PDI is unique in that it can execute both outside of a Hadoop cluster and within the nodes of a hadoop cluster.&nbsp;From outside a Hadoop cluster, PDI can extract data from or load data into Hadoop HDFS, Hive and HBase. When executed within
              the Hadoop cluster, PDI transformations can be used as Mapper and/or Reducer tasks, allowing PDI with Pentaho MapReduce to be used as visual programming tool for MapReduce.</p>
            <p class="p">These videos demonstrate using PDI to work with Hadoop from both inside and outside a Hadoop cluster.</p>
            <ul class="ul" id="5b4c0d9a-3565-443f-bc7f-d48540669de8__topic_working_with_big_data_and_hadoop__ul_rzp_k5p_zh">
              <li class="li">Loading Data into Hadoop from outside the Hadoop cluster is a 5-minute video that demonstrates moving data using a PDI job and transformation:
                <a class="xref external" target="_blank" rel="external nofollow" href="http://www.youtube.com/watch?v=Ylekzmd6TAc">http://www.youtube.com/watch?v=Ylekzmd6TAc</a> 
              </li>
              <li class="li">Use
                <a class="xref" title="Documentation/5.1/0Y0/0F0/000" rel="internal" href="http://help.pentaho.com/Draft_Content/Chantel_Archive/Chantel's_5.3_Work/Working_with_Big_Data_and_Hadoop_in_PDI_(archive)/000">Pentaho MapReduce</a>  to interactively design a data flow for a MapReduce job without writing scripts or code. Here is a 12 minute video that provides an overview of the process:
                <a class="xref external" target="_blank" rel="external nofollow"
                href="http://www.youtube.com/watch?v=KZe1UugxXcs">http://www.youtube.com/watch?v=KZe1UugxXcs</a> .</li>
            </ul>
          </div>
        </div>
      </div>
      <div class="related-links">
        <ul class="ullinks">
          <li class="link ulchildlink"><strong><a title="Documentation/5.1/0Y0/0F0/000" rel="internal" href="http://help.pentaho.com/Draft_Content/Chantel_Archive/Chantel's_5.3_Work/Working_with_Big_Data_and_Hadoop_in_PDI_(archive)/000">Pentaho MapReduce Workflow</a> </strong> </li>
          <li class="link ulchildlink"><strong><a title="Documentation/5.1/0Y0/0F0/010" rel="internal" href="http://help.pentaho.com/Draft_Content/Chantel_Archive/Chantel's_5.3_Work/Working_with_Big_Data_and_Hadoop_in_PDI_(archive)/010">PDI Hadoop Job Workflow</a> </strong> </li>
          <li
          class="link ulchildlink"><strong><a title="Documentation/5.1/0Y0/0F0/020" rel="internal" href="http://help.pentaho.com/Draft_Content/Chantel_Archive/Chantel's_5.3_Work/Working_with_Big_Data_and_Hadoop_in_PDI_(archive)/020">Hadoop to PDI Data Type Conversion</a> </strong> </li>
            <li class="link ulchildlink"><strong><a title="Documentation/5.1/130/050/020/000" rel="internal" href="http://help.pentaho.com/Documentation/5.4/0L0/130/050/020/000">Hadoop Hive-Specific SQL Limitations</a> </strong> </li>
            <li class="link ulchildlink"><strong><a title="Documentation/5.1/0Y0/0F0/040" rel="internal" href="http://help.pentaho.com/Draft_Content/Chantel_Archive/Chantel's_5.3_Work/Working_with_Big_Data_and_Hadoop_in_PDI_(archive)/040">Big Data Tutorials</a> </strong> </li>
        </ul>
      </div>
    </div>
  </body>
  <body target="toc">
    <ol>
      <li> <a href="#Overview" rel="internal">Overview</a>  </li>
      <li> <a href="#PDI's_Big_Data_Plugin" rel="internal">PDI's Big Data Plugin</a>  </li>
      <li> <a href="#Using_PDI_Outside_and_Inside_the_Hadoop_Cluster" rel="internal">Using PDI Outside and Inside the Hadoop Cluster</a>  </li>
    </ol>
  </body>
</content>